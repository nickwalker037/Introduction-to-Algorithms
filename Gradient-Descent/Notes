Model-fitting often reduces to optimization
- Ex. maximizing the likelihood of observed data over a family of generative models

What problem is gradient descent trying to solve?
- Unconstrained Optimization
    - for a given real-valued function f: Rn —> R defined on an n-dimensional Euclidean space, the goal is min f(x)
- but there are ways to extend gradient descent to handle constraints 
    - like projecting back to the feasible region

Unconstrained Optimization: 
- there are no constraints, and the only consideration is the objective function
- there are various ways to transform constrained problems into unconstrained ones
    - Lagrangian relaxation
    - Barrier functions

Think of it as we’re releasing a ball at some random point of the graph of function f, and the algorithm terminates at the final resting point of this ball (after gravity has done its work) (and you’re looking for a min value)
    - at each step, you use the derivative of f to decide which direction to move in
    - minimizing f using gradient descent may yield a different local minimum depending on the start position

Common Uses:
- Minimizing the Mean Squared Error (MSE) of a linear function
    - where the “error” or “residual” is the difference between the “prediction” for the i’th data point and the “correct answer”
    - minimizing the MSE is equivalent to the problem of maximizing (over linear functions) the likelihood of the data

Speeding up the process
- In many machine learning applications, one wants to use as much data as possible - as m grows larger, it’s possible to get more and more accuracy and/or use richer and richer models
- (1.) computing the gradient in every single iteration of gradient descent is just a sum of terms, one per data point, enabling easy parallelization
        - The data set can be spread over however many machines or cores are available, the summands can be computed independently, and then the results are aggregated together
- (2.) Stochastic Gradient Descent
        - ?? —> you start at diff random values and keep going until you run enough trials where the minimum is repeated (not improved upon) so you know it’s the lowest value in the set
